{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning notebook - Inverse Reinforcement Learning - Use rewards from sliced RL to imitate expert behavioural movements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import sys\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim \n",
    "import torchvision.transforms as transforms\n",
    "from collections import namedtuple, deque\n",
    "from torch import nn\n",
    "from gym import make\n",
    "import torch.optim as optim\n",
    "from numpy import save\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import pickle\n",
    "from typing import Optional\n",
    "import IPython\n",
    "from IPython.display import set_matplotlib_formats; set_matplotlib_formats('svg')\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "import os\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.utils import Net, Memory, Agent\n",
    "from Code.sliced_wasserstein_rewards import *\n",
    "from Code.plotting import preprocess_states, animate\n",
    "from Code.helper_functions import *\n",
    "from Code.pwil_rewarder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load expert trajectories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'Pendulum-v0'\n",
    "\n",
    "t1 = 'multi_diff_lengths_excl_1'\n",
    "traj1 = np.load('/Users/ilanasebag/Documents/Thesis_Code/RL_results/%s_exp_states_%s.npy'%(t1,environment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
    "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(environment, exp, seeds, simple = False, MMOT = False, wass_PWIL = False, PWIL = False):\n",
    "\n",
    "    env = gym.make(environment)\n",
    "    env.seed(seeds)\n",
    "\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0] * 5 #discretization  of the unique continuous action of the pendulum\n",
    "\n",
    "    agent = Agent(input_dim, output_dim, action_dim, environment)\n",
    "\n",
    "    training_records = []\n",
    "    running_reward, running_q = -1000, 0\n",
    "    \n",
    "\n",
    "    for i_ep in tqdm(range(800)):\n",
    "\n",
    "        rewards = []\n",
    "        new_states = []\n",
    "        old_states = []\n",
    "        action_indexes = []\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        #We fix the departure state \n",
    "        state = env.reset()\n",
    "        env.env.state = np.array([np.pi/2, 0.5])\n",
    "        env.env.last_u = None\n",
    "        state = env.env._get_obs()\n",
    "        \n",
    "        #to make it more robust we have to use :\n",
    "        #state = env.reset()\n",
    "        \n",
    "        for t in range(200):\n",
    "            action, action_index = agent.select_action(state)\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            old_states.append(state)\n",
    "\n",
    "            env.render()\n",
    "            #agent.store_transition(Transition(state, action_index, (reward + 8) / 8, state_))\n",
    "            state = state_\n",
    "            if agent.memory.isfull:\n",
    "                q = agent.update()\n",
    "                running_q = 0.99 * running_q + 0.01 * q\n",
    "\n",
    "            action_indexes.append(action_index)\n",
    "            rewards.append(reward)\n",
    "            new_states.append(state_)\n",
    "\n",
    "        states_tens = [torch.tensor(elt) for elt in old_states] #agent rollout \n",
    "        states_tens = torch.stack(states_tens).float()\n",
    "\n",
    "        \n",
    "        if MMOT is True :\n",
    "            rewards_multitask = rewarder_multi([states_tens, torch.tensor(exp[0]).float(), torch.tensor(exp[1]).float(), torch.tensor(exp[2]).float(), torch.tensor(exp[3]).float(), torch.tensor(exp[4]).float()], num_projections = 50)\n",
    "\n",
    "            \n",
    "        elif simple is True : \n",
    "            rewards_multitask = rewarder_multi([states_tens, torch.tensor(exp[0]).float()], num_projections = 50)\n",
    "            \n",
    "            \n",
    "        elif wass_PWIL is True : \n",
    "            pwil_exp = torch.tensor(concatenate_and_sample(exp)).float()\n",
    "            rewards_multitask = rewarder_multi([states_tens, pwil_exp], num_projections = 50)\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif PWIL is True : \n",
    "            pwilexp = concatenate_and_sample(exp)\n",
    "            \n",
    "            #states_tens = np.asarray(states_tens)\n",
    "            #rwd_class = PWILRewarder(pwilexp,states_tens, env)\n",
    "            #rewards_multitask = rwd_class.compute_reward()\n",
    "            #rewards_multitask = torch.from_numpy(rewards_multitask)\n",
    "            rewards_multitask = rp([states_tens, pwilexp])\n",
    "            \n",
    "\n",
    "            \n",
    "        for t in range(200):\n",
    "            rewards[t] = torch.exp(-5*rewards_multitask[t,0])\n",
    "            agent.store_transition(Transition(old_states[t], action_indexes[t], rewards[t], new_states[t]))\n",
    "\n",
    "        running_reward = running_reward * 0.9 + score * 0.1\n",
    "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
    "\n",
    "        print('Ep', i_ep, 'Average score:', running_reward, 'score of current env', score )\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return training_records\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad59eb1ecde4b7dbb81b27bd1dff50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4b175683a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraj1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrialpwil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMMOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwass_PWIL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPWIL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-510baf07cc28>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(environment, exp, seeds, simple, MMOT, wass_PWIL, PWIL)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m#rewards_multitask = rwd_class.compute_reward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m#rewards_multitask = torch.from_numpy(rewards_multitask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mrewards_multitask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates_tens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwilexp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_code/Code/pwil_rewarder.py\u001b[0m in \u001b[0;36mrp\u001b[0;34m(emp_measures, weights, device)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m#List of measures projected on the i_th projection   TODO: use a torch map to compute stuffs in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m#projected_measure_i = [measures_proj[m][:,i].reshape(-1,1) for m in range(num_measures)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mscore_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_code/Code/pwil_rewarder.py\u001b[0m in \u001b[0;36mr1\u001b[0;34m(emp_measures, weights, device)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp_measures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# emp_measures: list of measures, each being an array of dim n_support_points x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mnum_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mn_support_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memp_measures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    environment = 'Pendulum-v0'\n",
    "    exp = traj1\n",
    "    seeds = 1\n",
    "    trialpwil = main(environment, exp, seeds, simple = False, MMOT = False, wass_PWIL = False, PWIL = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([r.ep for r in trialpwil], [r.reward for r in trialpwil], color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
